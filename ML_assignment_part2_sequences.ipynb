{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification ML Assignment \n",
    "\n",
    "#### In this jupyter notebook, you will modify and run a machine learning model to classify human DNA sequences into coding vs intergenomic sequences. This script has several functions that are written for you, please do NOT modify any code unless it specifies to change it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 12:26:50.321989: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/projects/bgmp/malm/bioinfo/ml-assignment-Mahmoud-56/Sequence_classifier/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/projects/bgmp/malm/bioinfo/ml-assignment-Mahmoud-56/Sequence_classifier/lib/python3.11/site-packages/genomic_benchmarks/utils/datasets.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling1D,\n",
    "    MaxPooling1D,\n",
    ")\n",
    "\n",
    "# import wandb #uncomment if using weights and biases\n",
    "# from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "# import random\n",
    "\n",
    "from genomic_benchmarks.data_check import is_downloaded, info\n",
    "from genomic_benchmarks.models.tf import vectorize_layer\n",
    "\n",
    "from genomic_benchmarks.models.tf import get_basic_cnn_model_v0 as get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"demo_coding_vs_intergenomic_seqs\"\n",
    "VERSION = 0\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_downloaded(DATASET):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mdownload_dataset\u001b[49m(DATASET)\n\u001b[32m      4\u001b[39m info(DATASET)\n",
      "\u001b[31mNameError\u001b[39m: name 'download_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "if not is_downloaded(DATASET):\n",
    "    download_dataset(DATASET)\n",
    "\n",
    "info(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does anything strike you about the number of sequences? Why do you think this dataset was created with 100,000 200bp sequences from the human genome?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "CLASSES = ['coding_seqs', 'intergenomic_seqs']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "train_dset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    '/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/demo_coding_vs_intergenomic_seqs/train',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_names=CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are the sequences stored currently? Can you figure out if the below sequence is coding vs intergenomic sequence?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'CTGGAGAAGGCCCTGGAGTACCTGCGCCAGATATTCCGGCTCAGCGAAGCGCAGCTCAGGCAGTTCACACTCGCCTTGGGCACCACCCAGGATGAGAATGGAAAAAAGCAACTCCCCGACTGCATCGTGGGTGAGGACGGACTCATCCTTACGCCCCTGGGGCGGTACCAGATCATCAATGGGCTGCGAAGGTTTGAAAT'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dset)[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the sequences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'a', 't', 'g', 'c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.adapt(train_dset.map(lambda x, y: x))\n",
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text)-2, label\n",
    "\n",
    "train_ds = train_dset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How did the pre-processing change the sequence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([3, 3, 2, 3, 0, 1, 2, 2, 1, 3, 0, 2, 1, 2, 2, 3, 3, 0, 2, 3, 0, 2,\n",
       "       2, 2, 2, 1, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 1, 3, 3, 3, 0, 2, 3,\n",
       "       3, 2, 3, 3, 1, 2, 0, 2, 0, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 2, 2, 0,\n",
       "       3, 0, 3, 0, 2, 2, 0, 1, 3, 2, 3, 2, 2, 1, 2, 0, 2, 0, 1, 0, 3, 3,\n",
       "       1, 2, 2, 2, 0, 2, 1, 3, 0, 3, 2, 2, 2, 1, 2, 3, 1, 3, 2, 3, 0, 2,\n",
       "       0, 3, 2, 3, 0, 2, 3, 1, 2, 0, 3, 3, 3, 3, 0, 2, 0, 2, 2, 3, 0, 3,\n",
       "       0, 2, 2, 3, 3, 2, 3, 3, 0, 1, 2, 2, 3, 0, 1, 3, 0, 2, 0, 1, 2, 0,\n",
       "       0, 2, 2, 3, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 2, 2, 2,\n",
       "       3, 1, 2, 0, 2, 1, 1, 1, 1, 2, 0, 3, 0, 3, 3, 0, 0, 1, 2, 0, 2, 3,\n",
       "       0, 2])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_ds)[0][0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([2, 3, 3, 0, 0, 2, 1, 3, 1, 2, 0, 2, 2, 3, 3, 0, 3, 3, 0, 0, 2, 0,\n",
       "       2, 3, 1, 2, 3, 2, 2, 2, 0, 0, 2, 0, 1, 3, 0, 0, 3, 1, 1, 3, 0, 2,\n",
       "       3, 3, 1, 0, 3, 2, 3, 1, 0, 3, 2, 0, 1, 1, 0, 3, 2, 0, 2, 0, 3, 3,\n",
       "       2, 0, 2, 0, 3, 3, 3, 1, 2, 0, 1, 1, 2, 1, 2, 3, 2, 1, 0, 1, 3, 3,\n",
       "       1, 2, 0, 0, 2, 2, 3, 1, 1, 1, 1, 2, 0, 3, 3, 1, 3, 3, 3, 1, 2, 3,\n",
       "       3, 0, 0, 2, 2, 0, 3, 1, 1, 1, 1, 2, 1, 2, 2, 0, 0, 2, 3, 1, 3, 1,\n",
       "       2, 0, 3, 3, 3, 1, 1, 0, 1, 2, 1, 3, 0, 0, 2, 0, 1, 3, 1, 0, 3, 3,\n",
       "       1, 3, 3, 1, 2, 3, 3, 1, 2, 0, 3, 3, 2, 3, 0, 0, 0, 1, 2, 3, 0, 0,\n",
       "       2, 3, 1, 2, 3, 0, 2, 0, 3, 3, 3, 2, 2, 2, 1, 2, 3, 0, 3, 3, 2, 3,\n",
       "       0, 0])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    '/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/demo_coding_vs_intergenomic_seqs/test',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_names=CLASSES)\n",
    "\n",
    "test_ds = test_dset.map(vectorize_text)\n",
    "\n",
    "list(test_ds)[0][0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Recursive Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5, average=\"micro\")\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "acc = tf.metrics.BinaryAccuracy(threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove comments if using weights and biases\n",
    "\n",
    "# Start a run, tracking hyperparameters\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"sequence_classification_assignment\",\n",
    "\n",
    "#     # track hyperparameters and run metadata with wandb.config\n",
    "#     config={\n",
    "#         \"activation_2\": \"softmax\",\n",
    "#         \"optimizer\": \"adam\",\n",
    "#         \"loss\": \"binary_crossentropy\",\n",
    "#         \"metric\": \"accuracy\",\n",
    "#         \"epoch\": 10,\n",
    "#         \"batch_size\": 64\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_split_fn = lambda x: tf.strings.unicode_split(x, \"UTF-8\")\n",
    "vectorize_layer = TextVectorization(output_mode=\"int\", split=character_split_fn)\n",
    "onehot_layer = tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x, \"int64\"), vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           384       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40)                2600      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36049 (140.82 KB)\n",
      "Trainable params: 36049 (140.82 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn = tf.keras.Sequential()\n",
    "#LSTM is a type of RNN layer\n",
    "model_rnn.add(tf.keras.layers.Embedding(input_dim=6, output_dim=64, input_length=200))\n",
    "##instead of doing the one-hot encoding in this example, we used embeddings (code for a one_hot layer is provided above if you want to incorporate it)\n",
    "##input-dim = vocab size, outputdim=batch size, and inlength=sequence length\n",
    "model_rnn.add(tf.keras.layers.LSTM(64))\n",
    "model_rnn.add(tf.keras.layers.Dense(40,activation='relu'))\n",
    "model_rnn.add(tf.keras.layers.Dense(1))\n",
    "model_rnn.build((200,))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam', metrics = \"accuracy\")\n",
    "              #metrics= [config.metric]) # USE this if using weights and baises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1172/1172 [==============================] - 73s 61ms/step - loss: 0.5794 - accuracy: 0.6691\n",
      "Epoch 2/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.5926 - accuracy: 0.6536\n",
      "Epoch 3/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.5524 - accuracy: 0.6880\n",
      "Epoch 4/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.4532 - accuracy: 0.7728\n",
      "Epoch 5/10\n",
      "1172/1172 [==============================] - 72s 61ms/step - loss: 0.4109 - accuracy: 0.8020\n",
      "Epoch 6/10\n",
      "1172/1172 [==============================] - 72s 61ms/step - loss: 0.3686 - accuracy: 0.8282\n",
      "Epoch 7/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.3238 - accuracy: 0.8564\n",
      "Epoch 8/10\n",
      "1172/1172 [==============================] - 72s 61ms/step - loss: 0.2947 - accuracy: 0.8727\n",
      "Epoch 9/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.2813 - accuracy: 0.8780\n",
      "Epoch 10/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.2683 - accuracy: 0.8858\n"
     ]
    }
   ],
   "source": [
    "history = model_rnn.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS, batch_size=64)\n",
    "\n",
    "## Run this model fit command if using weights and biases\n",
    "# history = model_rnn.fit(\n",
    "#     train_ds,\n",
    "#     epochs=EPOCHS, batch_size=config.batch_size, callbacks=[\n",
    "#                       WandbMetricsLogger(log_freq=5),\n",
    "#                       WandbModelCheckpoint(\"models\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 9s 22ms/step - loss: 0.2894 - accuracy: 0.8886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28937453031539917, 0.8886399865150452]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code and Explore!\n",
    "\n",
    "In this exploration, you are **required to create three different neural networks to solve the above problem**. Creating a model can include 1) fundamentally changing the type of layers (ex: recursive layers to convolutional layers), adding additional layers including pooling and activation layers, or changing the functions (loss, optimizer). Your new models do **not** have to be better than the recursive model shown above; however, you **must explain what you did and why you decided to try something out**. You may also change hyperparameters (batch size, epoch number), but please make some major structure changes in addition to hyperparameter changes. \n",
    "\n",
    "Most importantly, have fun and be curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspiration: \n",
    "https://github.com/Jawwad-Fida/DNA-sequence-classification-by-Deep-Neural-Network\n",
    "\n",
    "https://colab.research.google.com/github/google/nucleus/blob/master/nucleus/examples/dna_sequencing_error_correction.ipynb\n",
    "\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "https://github.com/const-ae/Neural_Network_DNA_Demo/blob/master/nn_for_sequence_data.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain your change here (what you did and why you tried that out)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain your change here (what you did and why you tried that out)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain your change here (what you did and why you tried that out)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are any of your models more successful than model_rnn? Explain why**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
