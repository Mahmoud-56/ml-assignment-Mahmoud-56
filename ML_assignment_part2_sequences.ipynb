{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification ML Assignment \n",
    "\n",
    "#### In this jupyter notebook, you will modify and run a machine learning model to classify human DNA sequences into coding vs intergenomic sequences. This script has several functions that are written for you, please do NOT modify any code unless it specifies to change it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 12:26:50.321989: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/projects/bgmp/malm/bioinfo/ml-assignment-Mahmoud-56/Sequence_classifier/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/projects/bgmp/malm/bioinfo/ml-assignment-Mahmoud-56/Sequence_classifier/lib/python3.11/site-packages/genomic_benchmarks/utils/datasets.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling1D,\n",
    "    MaxPooling1D,\n",
    ")\n",
    "\n",
    "# import wandb #uncomment if using weights and biases\n",
    "# from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "# import random\n",
    "\n",
    "from genomic_benchmarks.data_check import is_downloaded, info\n",
    "from genomic_benchmarks.models.tf import vectorize_layer\n",
    "\n",
    "from genomic_benchmarks.models.tf import get_basic_cnn_model_v0 as get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"demo_coding_vs_intergenomic_seqs\"\n",
    "VERSION = 0\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_downloaded(DATASET):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mdownload_dataset\u001b[49m(DATASET)\n\u001b[32m      4\u001b[39m info(DATASET)\n",
      "\u001b[31mNameError\u001b[39m: name 'download_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "if not is_downloaded(DATASET):\n",
    "    download_dataset(DATASET)\n",
    "\n",
    "info(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does anything strike you about the number of sequences? Why do you think this dataset was created with 100,000 200bp sequences from the human genome?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large number of sequences ensures the model has enough data to learn patterns without overfitting, and the 200bp length is long enough to capture important features of coding and non-coding regions without being too computationally heavy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "CLASSES = ['coding_seqs', 'intergenomic_seqs']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "train_dset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    '/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/demo_coding_vs_intergenomic_seqs/train',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_names=CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How are the sequences stored currently? Can you figure out if the below sequence is coding vs intergenomic sequence?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences are stored as usual DNA sequences as strings. We can't directly tell coding vs intergenomic sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'CTGGAGAAGGCCCTGGAGTACCTGCGCCAGATATTCCGGCTCAGCGAAGCGCAGCTCAGGCAGTTCACACTCGCCTTGGGCACCACCCAGGATGAGAATGGAAAAAAGCAACTCCCCGACTGCATCGTGGGTGAGGACGGACTCATCCTTACGCCCCTGGGGCGGTACCAGATCATCAATGGGCTGCGAAGGTTTGAAAT'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dset)[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the sequences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'a', 't', 'g', 'c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.adapt(train_dset.map(lambda x, y: x))\n",
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text)-2, label\n",
    "\n",
    "train_ds = train_dset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How did the pre-processing change the sequence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences are converted into integer sequences using a TextVectorization layer. This layer maps each nucleotide (A, T, G, C) to an integer and UNK to unknown nucleotides. The adapt method is used to fit the vectorization layer on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([3, 3, 2, 3, 0, 1, 2, 2, 1, 3, 0, 2, 1, 2, 2, 3, 3, 0, 2, 3, 0, 2,\n",
       "       2, 2, 2, 1, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 1, 3, 3, 3, 0, 2, 3,\n",
       "       3, 2, 3, 3, 1, 2, 0, 2, 0, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 2, 2, 0,\n",
       "       3, 0, 3, 0, 2, 2, 0, 1, 3, 2, 3, 2, 2, 1, 2, 0, 2, 0, 1, 0, 3, 3,\n",
       "       1, 2, 2, 2, 0, 2, 1, 3, 0, 3, 2, 2, 2, 1, 2, 3, 1, 3, 2, 3, 0, 2,\n",
       "       0, 3, 2, 3, 0, 2, 3, 1, 2, 0, 3, 3, 3, 3, 0, 2, 0, 2, 2, 3, 0, 3,\n",
       "       0, 2, 2, 3, 3, 2, 3, 3, 0, 1, 2, 2, 3, 0, 1, 3, 0, 2, 0, 1, 2, 0,\n",
       "       0, 2, 2, 3, 0, 0, 0, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 0, 2, 2, 2,\n",
       "       3, 1, 2, 0, 2, 1, 1, 1, 1, 2, 0, 3, 0, 3, 3, 0, 0, 1, 2, 0, 2, 3,\n",
       "       0, 2])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_ds)[0][0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
       "array([2, 3, 3, 0, 0, 2, 1, 3, 1, 2, 0, 2, 2, 3, 3, 0, 3, 3, 0, 0, 2, 0,\n",
       "       2, 3, 1, 2, 3, 2, 2, 2, 0, 0, 2, 0, 1, 3, 0, 0, 3, 1, 1, 3, 0, 2,\n",
       "       3, 3, 1, 0, 3, 2, 3, 1, 0, 3, 2, 0, 1, 1, 0, 3, 2, 0, 2, 0, 3, 3,\n",
       "       2, 0, 2, 0, 3, 3, 3, 1, 2, 0, 1, 1, 2, 1, 2, 3, 2, 1, 0, 1, 3, 3,\n",
       "       1, 2, 0, 0, 2, 2, 3, 1, 1, 1, 1, 2, 0, 3, 3, 1, 3, 3, 3, 1, 2, 3,\n",
       "       3, 0, 0, 2, 2, 0, 3, 1, 1, 1, 1, 2, 1, 2, 2, 0, 0, 2, 3, 1, 3, 1,\n",
       "       2, 0, 3, 3, 3, 1, 1, 0, 1, 2, 1, 3, 0, 0, 2, 0, 1, 3, 1, 0, 3, 3,\n",
       "       1, 3, 3, 1, 2, 3, 3, 1, 2, 0, 3, 3, 2, 3, 0, 0, 0, 1, 2, 3, 0, 0,\n",
       "       2, 3, 1, 2, 3, 0, 2, 0, 3, 3, 3, 2, 2, 2, 1, 2, 3, 0, 3, 3, 2, 3,\n",
       "       0, 0])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    '/projects/bgmp/shared/Bi625/ML_Assignment/Datasets/demo_coding_vs_intergenomic_seqs/test',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_names=CLASSES)\n",
    "\n",
    "test_ds = test_dset.map(vectorize_text)\n",
    "\n",
    "list(test_ds)[0][0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Recursive Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = tfa.metrics.F1Score(num_classes=1, threshold=0.5, average=\"micro\")\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "acc = tf.metrics.BinaryAccuracy(threshold=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove comments if using weights and biases\n",
    "\n",
    "# Start a run, tracking hyperparameters\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"sequence_classification_assignment\",\n",
    "\n",
    "#     # track hyperparameters and run metadata with wandb.config\n",
    "#     config={\n",
    "#         \"activation_2\": \"softmax\",\n",
    "#         \"optimizer\": \"adam\",\n",
    "#         \"loss\": \"binary_crossentropy\",\n",
    "#         \"metric\": \"accuracy\",\n",
    "#         \"epoch\": 10,\n",
    "#         \"batch_size\": 64\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_split_fn = lambda x: tf.strings.unicode_split(x, \"UTF-8\")\n",
    "vectorize_layer = TextVectorization(output_mode=\"int\", split=character_split_fn)\n",
    "onehot_layer = tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x, \"int64\"), vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 64)           384       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 40)                2600      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36049 (140.82 KB)\n",
      "Trainable params: 36049 (140.82 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn = tf.keras.Sequential()\n",
    "#LSTM is a type of RNN layer\n",
    "model_rnn.add(tf.keras.layers.Embedding(input_dim=6, output_dim=64, input_length=200))\n",
    "##instead of doing the one-hot encoding in this example, we used embeddings (code for a one_hot layer is provided above if you want to incorporate it)\n",
    "##input-dim = vocab size, outputdim=batch size, and inlength=sequence length\n",
    "model_rnn.add(tf.keras.layers.LSTM(64))\n",
    "model_rnn.add(tf.keras.layers.Dense(40,activation='relu'))\n",
    "model_rnn.add(tf.keras.layers.Dense(1))\n",
    "model_rnn.build((200,))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam', metrics = \"accuracy\")\n",
    "              #metrics= [config.metric]) # USE this if using weights and baises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1172/1172 [==============================] - 73s 61ms/step - loss: 0.5794 - accuracy: 0.6691\n",
      "Epoch 2/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.5926 - accuracy: 0.6536\n",
      "Epoch 3/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.5524 - accuracy: 0.6880\n",
      "Epoch 4/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.4532 - accuracy: 0.7728\n",
      "Epoch 5/10\n",
      "1172/1172 [==============================] - 72s 61ms/step - loss: 0.4109 - accuracy: 0.8020\n",
      "Epoch 6/10\n",
      "1172/1172 [==============================] - 72s 61ms/step - loss: 0.3686 - accuracy: 0.8282\n",
      "Epoch 7/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.3238 - accuracy: 0.8564\n",
      "Epoch 8/10\n",
      "1172/1172 [==============================] - 72s 61ms/step - loss: 0.2947 - accuracy: 0.8727\n",
      "Epoch 9/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.2813 - accuracy: 0.8780\n",
      "Epoch 10/10\n",
      "1172/1172 [==============================] - 71s 61ms/step - loss: 0.2683 - accuracy: 0.8858\n"
     ]
    }
   ],
   "source": [
    "history = model_rnn.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS, batch_size=64)\n",
    "\n",
    "## Run this model fit command if using weights and biases\n",
    "# history = model_rnn.fit(\n",
    "#     train_ds,\n",
    "#     epochs=EPOCHS, batch_size=config.batch_size, callbacks=[\n",
    "#                       WandbMetricsLogger(log_freq=5),\n",
    "#                       WandbModelCheckpoint(\"models\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 9s 22ms/step - loss: 0.2894 - accuracy: 0.8886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28937453031539917, 0.8886399865150452]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code and Explore!\n",
    "\n",
    "In this exploration, you are **required to create three different neural networks to solve the above problem**. Creating a model can include 1) fundamentally changing the type of layers (ex: recursive layers to convolutional layers), adding additional layers including pooling and activation layers, or changing the functions (loss, optimizer). Your new models do **not** have to be better than the recursive model shown above; however, you **must explain what you did and why you decided to try something out**. You may also change hyperparameters (batch size, epoch number), but please make some major structure changes in addition to hyperparameter changes. \n",
    "\n",
    "Most importantly, have fun and be curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspiration: \n",
    "https://github.com/Jawwad-Fida/DNA-sequence-classification-by-Deep-Neural-Network\n",
    "\n",
    "https://colab.research.google.com/github/google/nucleus/blob/master/nucleus/examples/dna_sequencing_error_correction.ipynb\n",
    "\n",
    "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "\n",
    "https://www.tensorflow.org/text/tutorials/text_classification_rnn\n",
    "\n",
    "https://github.com/const-ae/Neural_Network_DNA_Demo/blob/master/nn_for_sequence_data.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added:\n",
    "- Conv1D Layer: A 1D convolutional layer with 64 filters and a kernel size of 3. This can potentially help in capturing local patterns in the sequencse, giving the model higher accuracy. \n",
    "- MaxPooling1D Layer: A max pooling layer with a pool size of 2. This can help the model in reducing the dimensionality of the sequence, keeping only the most important features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1172/1172 [==============================] - 47s 38ms/step - loss: 0.4073 - accuracy: 0.8050\n",
      "Epoch 2/10\n",
      "1172/1172 [==============================] - 45s 38ms/step - loss: 0.3334 - accuracy: 0.8532\n",
      "Epoch 3/10\n",
      "1172/1172 [==============================] - 45s 38ms/step - loss: 0.3170 - accuracy: 0.8631\n",
      "Epoch 4/10\n",
      "1172/1172 [==============================] - 45s 38ms/step - loss: 0.3049 - accuracy: 0.8681\n",
      "Epoch 5/10\n",
      "1172/1172 [==============================] - 45s 38ms/step - loss: 0.2937 - accuracy: 0.8738\n",
      "Epoch 6/10\n",
      "1172/1172 [==============================] - 44s 37ms/step - loss: 0.2835 - accuracy: 0.8777\n",
      "Epoch 7/10\n",
      "1172/1172 [==============================] - 43s 37ms/step - loss: 0.2727 - accuracy: 0.8829\n",
      "Epoch 8/10\n",
      "1172/1172 [==============================] - 43s 37ms/step - loss: 0.2652 - accuracy: 0.8869\n",
      "Epoch 9/10\n",
      "1172/1172 [==============================] - 43s 37ms/step - loss: 0.2562 - accuracy: 0.8909\n",
      "Epoch 10/10\n",
      "1172/1172 [==============================] - 43s 37ms/step - loss: 0.2492 - accuracy: 0.8947\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.2564 - accuracy: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2563627362251282, 0.8954799771308899]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Embedding\n",
    "\n",
    "# Build the model with a CNN layer\n",
    "model_cnn = tf.keras.Sequential([\n",
    "    Embedding(input_dim=6, output_dim=64, input_length=200),  # Embedding layer\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),     # 1D Convolutional layer\n",
    "    MaxPooling1D(pool_size=2),                                # Max pooling layer\n",
    "    LSTM(64),                                                # LSTM layer\n",
    "    Dense(40, activation='relu'),                            # Dense layer\n",
    "    Dense(1)                                                 # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_cnn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history_cnn = model_cnn.fit(train_ds, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Evaluate the model\n",
    "model_cnn.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I replaced the standard LSTM with a bidirectional LSTM. Unlike a standard LSTM which processes sequences in one direction (forward), a bidirectional LSTM processes sequences in both forward and backward directions. This allows the model to capture context from both ends of the sequence, potentially increasing the model accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1172/1172 [==============================] - 91s 76ms/step - loss: 0.5440 - accuracy: 0.7182\n",
      "Epoch 2/10\n",
      "1172/1172 [==============================] - 89s 76ms/step - loss: 0.5046 - accuracy: 0.7397\n",
      "Epoch 3/10\n",
      "1172/1172 [==============================] - 89s 75ms/step - loss: 0.4538 - accuracy: 0.7721\n",
      "Epoch 4/10\n",
      "1172/1172 [==============================] - 89s 76ms/step - loss: 0.4246 - accuracy: 0.7942\n",
      "Epoch 5/10\n",
      "1172/1172 [==============================] - 89s 76ms/step - loss: 0.4050 - accuracy: 0.8046\n",
      "Epoch 6/10\n",
      "1172/1172 [==============================] - 88s 75ms/step - loss: 0.3519 - accuracy: 0.8365\n",
      "Epoch 7/10\n",
      "1172/1172 [==============================] - 89s 76ms/step - loss: 0.3177 - accuracy: 0.8605\n",
      "Epoch 8/10\n",
      "1172/1172 [==============================] - 87s 74ms/step - loss: 0.2854 - accuracy: 0.8787\n",
      "Epoch 9/10\n",
      "1172/1172 [==============================] - 86s 74ms/step - loss: 0.2468 - accuracy: 0.8982\n",
      "Epoch 10/10\n",
      "1172/1172 [==============================] - 87s 74ms/step - loss: 0.2203 - accuracy: 0.9105\n",
      "391/391 [==============================] - 11s 25ms/step - loss: 0.2252 - accuracy: 0.9168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2252350151538849, 0.9168400168418884]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with Bidirectional LSTM\n",
    "model_bilstm = tf.keras.Sequential([\n",
    "    Embedding(input_dim=6, output_dim=64, input_length=200),  \n",
    "    Bidirectional(LSTM(64)),                                 \n",
    "    Dense(40, activation='relu'),                            \n",
    "    Dense(1)                                                \n",
    "])\n",
    "\n",
    "\n",
    "model_bilstm.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                     optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "history_bilstm = model_bilstm.fit(train_ds, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "model_bilstm.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decreased the learning rate from 0.001 to 0.0001 hoping to find a better balance between training speed and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1172/1172 [==============================] - 68s 57ms/step - loss: 0.5073 - accuracy: 0.7233\n",
      "Epoch 2/10\n",
      "1172/1172 [==============================] - 67s 57ms/step - loss: 0.4730 - accuracy: 0.7535\n",
      "Epoch 3/10\n",
      "1172/1172 [==============================] - 66s 57ms/step - loss: 0.4710 - accuracy: 0.7542\n",
      "Epoch 4/10\n",
      "1172/1172 [==============================] - 66s 57ms/step - loss: 0.4689 - accuracy: 0.7550\n",
      "Epoch 5/10\n",
      "1172/1172 [==============================] - 67s 57ms/step - loss: 0.4663 - accuracy: 0.7566\n",
      "Epoch 6/10\n",
      "1172/1172 [==============================] - 67s 57ms/step - loss: 0.4485 - accuracy: 0.7706\n",
      "Epoch 7/10\n",
      "1172/1172 [==============================] - 66s 56ms/step - loss: 0.4358 - accuracy: 0.7828\n",
      "Epoch 8/10\n",
      "1172/1172 [==============================] - 66s 56ms/step - loss: 0.4273 - accuracy: 0.7894\n",
      "Epoch 9/10\n",
      "1172/1172 [==============================] - 66s 56ms/step - loss: 0.4223 - accuracy: 0.7947\n",
      "Epoch 10/10\n",
      "1172/1172 [==============================] - 66s 56ms/step - loss: 0.4181 - accuracy: 0.7985\n",
      "391/391 [==============================] - 8s 21ms/step - loss: 0.4177 - accuracy: 0.7826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41771820187568665, 0.7825599908828735]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model (same as the original model)\n",
    "model_lr = tf.keras.Sequential([\n",
    "    Embedding(input_dim=6, output_dim=64, input_length=200),  # Embedding layer\n",
    "    LSTM(64),                                                # LSTM layer\n",
    "    Dense(40, activation='relu'),                            # Dense layer\n",
    "    Dense(1)                                                 # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model with a custom learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)    # Lower learning rate\n",
    "model_lr.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                 optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history_lr = model_lr.fit(train_ds, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Evaluate the model\n",
    "model_lr.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Are any of your models more successful than model_rnn? Explain why**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes. The first two models did perform better than model_rnn (0.89 and 0.91). It appears that having a bidirectional LSTM was the most helpful and this can be due to the additional context given to the model using this approach by reading the sequence in both directions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
